# -*- coding: utf-8 -*-
"""Different Optimizers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KvHwdBRFsWmAocVHkiJtNMQsRYV9WLur

<img style="float: centre;" src="https://drive.google.com/uc?id=1ZVzMRu99qZNwgAM0myg0Ub0-I9hVcT9F">

# Different Optimizer functions
"""

import torch
import torchvision 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
from torch.autograd import Variable
import time

trainset = dsets.FashionMNIST(root='./my_data', train=True,
                                        download=True, transform=transforms.ToTensor())


testset = dsets.FashionMNIST(root='./my_data', train=False,
                                       download=True, transform=transforms.ToTensor())

batch_size = 100 
num_epochs = 20
print("Number of epochs : ",num_epochs)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=4)

testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=4)

# SIMPLE MODEL DEFINITION
class Simple_MLP(nn.Module):
    def __init__(self, size_list):
        super(Simple_MLP, self).__init__()
        layers = []
        self.size_list = size_list
        for i in range(len(size_list) - 2):
            layers.append(nn.Linear(size_list[i],size_list[i+1]))
            layers.append(nn.ReLU())
        layers.append(nn.Linear(size_list[-2], size_list[-1]))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        x = x.view(-1, self.size_list[0]) # Flatten the input
        return self.net(x)

model = Simple_MLP([784, 100, 10])

criterion = nn.CrossEntropyLoss()

learning_rate = 0.001

cuda = torch.cuda.is_available()
device = torch.device("cuda" if cuda else "cpu")

def train_epoch(model,train_loader,optimizer, criterion):
    model.train()
    model.to(device)
    running_loss = 0.0
    
    start_time = time.time()
    for batch_idx, (data, target) in enumerate(train_loader):   
        optimizer.zero_grad()   
        data = data.to(device)
        target = target.long().to(device)

        outputs = model(data)
        loss = criterion(outputs, target)
        running_loss += loss.item()

        loss.backward()
        optimizer.step()
    
    end_time = time.time()
    
    running_loss /= len(train_loader)
    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')
    return running_loss

def test_model(model, test_loader, criterion):
    with torch.no_grad():
        model.eval()
        model.to(device)

        running_loss = 0.0
        total_predictions = 0.0
        correct_predictions = 0.0

        for batch_idx, (data, target) in enumerate(test_loader):   
            data = data.to(device)
            target = target.long().to(device)

            outputs = model(data)

            _, predicted = torch.max(outputs.data, 1)
            total_predictions += target.size(0)
            correct_predictions += (predicted == target).sum().item()

            loss = criterion(outputs, target).detach()
            running_loss += loss.item()


        running_loss /= len(test_loader)
        acc = (correct_predictions/total_predictions)*100.0
        print('Testing Loss: ', running_loss)
        print('Testing Accuracy: ', acc, '%')
        return running_loss, acc

"""# 1. Training with ADAM"""

optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), eps=1e-08, lr = learning_rate)
print(optimizer)

loss_train_adam = []
loss_acc_test_adam = []

for i in range(num_epochs):
  trep = train_epoch(model, trainloader, optimizer, criterion)
  temo = test_model(model, testloader, criterion)
  loss_train_adam.append(trep)
  loss_acc_test_adam.append(temo)

test_loss_adam = []
test_accuracy_adam =[]

for i in loss_acc_test_adam:
  test_loss_adam.append(i[0])
  test_accuracy_adam.append(i[1])

"""# 2. Training with SGD"""

optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)
print(optimizer)

loss_train_sgd = []
loss_acc_test_sgd = []

for i in range(num_epochs):
  trep = train_epoch(model, trainloader, optimizer, criterion)
  temo = test_model(model, testloader, criterion)
  loss_train_sgd.append(trep)
  loss_acc_test_sgd.append(temo)

test_loss_sgd = []
test_accuracy_sgd =[]

for i in loss_acc_test_sgd:
  test_loss_sgd.append(i[0])
  test_accuracy_sgd.append(i[1])

"""# 3. Training with RMSProp"""

optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate, alpha=0.99, eps=1e-06, centered=True)
print(optimizer)

loss_train_rms = []
loss_acc_test_rms = []

for i in range(num_epochs):
  trep = train_epoch(model, trainloader, optimizer, criterion)
  temo = test_model(model, testloader, criterion)
  loss_train_rms.append(trep)
  loss_acc_test_rms.append(temo)

test_loss_rms = []
test_accuracy_rms =[]

for i in loss_acc_test_rms:
  test_loss_rms.append(i[0])
  test_accuracy_rms.append(i[1])

"""# Plotting Training Losses"""

import matplotlib.pyplot as plt

epoch=list(range(1,num_epochs+1))
plt.figure(figsize=(12,12))
plt.plot(epoch, loss_train_adam, label = 'Adam ', alpha = 0.5)
plt.plot(epoch, loss_train_sgd,label = 'SGD', alpha = 0.5)
plt.plot(epoch, loss_train_rms, label = 'RMSProp', alpha = 0.5)


plt.title("Trainnig losses over epochs")
plt.xlabel("Epochs")
plt.ylabel("Training Losses")
plt.legend()
plt.show()

"""# Plotting Test Losses"""

plt.figure(figsize=(12,12))
plt.plot(epoch, test_loss_adam, label = 'Adam ', alpha = 0.5)
plt.plot(epoch, test_loss_sgd,label = 'SGD', alpha = 0.5)
plt.plot(epoch, test_loss_rms, label = 'RMSProp', alpha = 0.5)


plt.title("Test losses over epochs")
plt.xlabel("Epochs")
plt.ylabel("Test Losses")
plt.legend()
plt.show()

"""# Plotting Test Accuracies"""

plt.figure(figsize=(12,12))
plt.plot(epoch, test_accuracy_adam, label = 'Adam ', alpha = 0.5)
plt.plot(epoch, test_accuracy_sgd,label = 'SGD', alpha = 0.5)
plt.plot(epoch, test_accuracy_rms, label = 'RMSProp', alpha = 0.5)


plt.title("Test accuracy over epochs")
plt.xlabel("Epochs")
plt.ylabel("Test Accuracy")
plt.legend()
plt.show()

"""# The End"""